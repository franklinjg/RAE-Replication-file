# The following code installs the required packages. 

import sys         # These are Python libraries so no need to install them.
import subprocess

# Function to install packages
def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# List of required packages
required_packages = [
    "numpy",
    "pandas",
    "sympy",
    "scipy",
    "matplotlib",
    "requests",
    "pyreadr"
]

# Try to import the packages, install if they're not found
for package in required_packages:
    try:
        __import__(package)
    except ImportError:
        install_package(package)

# Now we can safely import the packages
import numpy as np
import pandas as pd
from sympy import Matrix
from scipy.optimize import minimize
from scipy.special import comb, logsumexp
import matplotlib.pyplot as plt
import requests
import pyreadr
import os
import tempfile
from scipy.linalg import solve


np.random.seed(4862)         # I use this seed throughout my code so if you want to replicate my results use this one. 


# Arrays to store the results
x1 = []
x2 = []
x3 = []
x4 = []
x5 = []
x6 = []
x7 = []
x8 = []
x9 = []
x10 = []
x11 = []
x12 = []
x13 = []
x14 = []
x15 = []
x16 = []
x17 = []
x18 = []
x19 = []
x20 = []
x21 = []
x22 = []
x23 = []
x24 = []
x25 = []
x26 = []
x27 = []
x28 = []
x29 = []
x30 = []



############################################################################################################################################################################

# Generating the constants and storing them 

phi = 0.20 
Psi_values = np.arange(0.05, 0.14, 0.01)    # Making the interval smallers will give you the same graph (I tried it)            
X = 5.0                                     # This is the budget of the interest groups which is used in the computation of the optimal transfers.
q = 0.5                                     # This is the value of q which is used in the computation of the pivotal probabilities. (q-rule).      
                                            # This is the value of phi* which is used in the computation of the Katz-Bonacich centrality vector.
vmax = 1.0                                  # This is the maximum value of the diagonal elements of the matrix V.

# Function to compute phistar for a given Psi
def phistar(Psi):
    return 2 * Psi * phi


# Number of Legislators. 

n = 429

############################################################################################################################################################################

def store_matrix_entries(matrix):
    # Create an empty dictionary to hold the entries of the matrix
    stored_entries = {}

    # Determine the size of the matrix (assuming it's square)
    n = matrix.shape[0]

    # Loop over each row of the matrix
    for i in range(n):
        # Loop over each column of the matrix
        for j in range(n):
            # Create a unique identifier for each element of the matrix
            # This identifier follows the format "g_rowcolumn", starting from 1 (e.g., "g_11" for the top-left element)
            identifier = f"g_{i+1}{j+1}"

            # Store each element of the matrix in the dictionary with its unique identifier as the key
            stored_entries[identifier] = matrix[i, j]

    # Return the dictionary containing all matrix entries
    # This dictionary can be used for quick reference to any element of the matrix by its "g_rowcolumn" identifier
    return stored_entries

# Example usage:
# Assuming you have a numpy array 'matrix' which you want to store
# entries_dict = store_matrix_entries(matrix)
# Now 'entries_dict' will hold all the values of the matrix,
# which you can access using their respective identifiers.


# URL of the raw R data file on GitHub
url = 'https://raw.githubusercontent.com/franklinjg/RAE/29da2338741e37a621442490179455c570402bf4/G_party.rda'

# Make a request to get the content of the .rda file
response = requests.get(url)
if response.status_code == 200:
    # Create a temporary file
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(response.content)
        tmp_file_path = tmp_file.name

    # Read the R data file from the temporary file
    result = pyreadr.read_r(tmp_file_path)

    # Optionally delete the temporary file if you're done with it
    os.remove(tmp_file_path)

    # Now you can use 'result' as a normal result object from pyreadr
    print(result.keys())  # For example, to print the keys of the loaded R data

    # Assuming 'G_party' is the key for the data in the .rda file
    G_party = result['G_party']

    # Extract the 429x429 block from the DataFrame and convert it to a numpy matrix
    Gn = G_party.iloc[0:429, 0:429].values

    # Transpose the numpy matrix G
    G_transpose = Gn.T

    # Storing matrix entries in a dictionary from the extracted block
    entries_dict = store_matrix_entries(Gn)
else:
    print("Failed to fetch the file: Status code", response.status_code)

############################################################################################################################################################################

# Function to generate an n x n diagonal matrix V with non-zero diagonal elements
# and save the diagonal elements as v1, v2, ..., vn
def generate_and_save_diagonal_matrix(n):
    """
    This function generates an n x n diagonal matrix V where each diagonal element
    is randomly selected from a set of specified non-zero values.
    It then saves this matrix to a file and also returns the matrix along with the diagonal elements.
    
    Parameters:
    n (int): The size of the matrix (number of rows/columns)
    
    Returns:
    tuple: A tuple containing the matrix V, the filename it was saved as,
           a dictionary of diagonal elements, and an array of diagonal values.
    """
    
    # Define the possible non-zero values for the diagonal elements
    possible_values_diagonal = [-1, 1, 0.5, -0.5]
    
    # Create an n x n matrix filled with zeros
    V = np.zeros((n, n))
    
    # Set each diagonal element to a randomly chosen value from the possible values
    for i in range(n):
        V[i, i] = np.random.choice(possible_values_diagonal)
        
        # If zero is randomly chosen, re-pick until a non-zero is chosen
        while V[i, i] == 0:
            V[i, i] = np.random.choice(possible_values_diagonal)
    
    # Save the matrix to a binary file in NumPy .npy format
    filename = f'V{n}.npy'
    np.save(filename, V)
    
    # Extract the diagonal elements into a separate array
    v_elements = np.diag(V)
    
    # Create a dictionary to hold the diagonal elements with their corresponding variable names
    v_dict = {f'v{i+1}': v_elements[i] for i in range(n)}
    
    # Return the matrix, filename, dictionary, and array of diagonal values
    return V, filename, v_dict, v_elements

# Generate the matrix and get the diagonal values
V, filename, v_dict, v_values = generate_and_save_diagonal_matrix(n)  # Now v_values is defined


# Iterate over Psi to compute and analyze Katz-Bonacich centrality vectors
for Psi in Psi_values:
    # Create identity matrix
    I = np.eye(n)
    
    # Calculate the phistar value for current Psi
    phistar_value = phistar(Psi)
    
    # Formulate the matrix for inversion based on phistar_value and the network matrix G
    matrix_to_invert = I - phistar_value * Gn
    
    # Check the invertibility of the matrix
    is_invertible = np.linalg.det(matrix_to_invert) != 0
    
    if is_invertible:
        # Invert the matrix to compute the Katz-Bonacich centrality vector
        inverse_matrix = np.linalg.inv(matrix_to_invert)
        
        # Multiply the inverse with a vector of ones to obtain the centrality vector
        katz_bonacich_vector = inverse_matrix.dot(np.ones(n))
        
        # Verify that all centrality values are positive, as required
        all_positive = np.all(katz_bonacich_vector > 0)
    else:
        # If not invertible, the vector cannot be computed
        all_positive = False

    # Determine if Assumption 2 is satisfied
    assumption_2_holds = is_invertible and all_positive
    # Output the result for the current Psi value
    print(f"Psi: {Psi}, Does Assumption 2 hold?: {assumption_2_holds}")



############################################################################################################################################################################

def check_inequality(Psi, vmax, phi, X):
    value_inside = vmax + phi + np.log(2*X)
    result = Psi * value_inside
    print(f"For Psi = {Psi}, computed result: {result}")  
    return result < 0.5

# Loop over each Psi and check if the inequality holds
for Psi in Psi_values:
    inequality_holds = check_inequality(Psi, vmax, phi, X)
    print(f"Does the inequality hold strictly for Psi = {Psi}? : {inequality_holds}")

############################################################################################################################################################################

# Function to calculate pivotal probabilities for a given strategy profile x
def calculate_pivotal_probabilities(x):
    # Compute half of the size of x for the pivotal event's threshold
    half_n = (len(x) - 1) // 2
    # Initialize an array to hold pivotal probabilities for each player
    pivotal_probs = np.zeros(len(x))
    # Define a small positive value to avoid log(0) in calculations
    epsilon = 1e-10
    # Precompute logarithms of combinatorial terms for efficiency
    log_comb = np.array([np.log(comb(len(x) - 1, j)) for j in range(half_n)])
    
    # Calculate the probability of being pivotal for each player
    for i in range(len(x)):
        # Exclude the current player's probability when considering others
        others = np.concatenate((x[:i], x[i+1:]))
        # Ensure probabilities are within (0,1) using epsilon
        others = np.clip(others, epsilon, 1 - epsilon)
        # Calculate log probabilities for being pivotal based on others' strategies
        if i != len(x) - 1:
            log_probs = log_comb + np.log(others[:half_n]) + np.log(1 - others[half_n:])
        else:
            # Use the same computation for the last player
            log_probs = log_comb + np.log(others[:half_n]) + np.log(1 - others[half_n:])
        # Sum the exponentiated log probabilities for numerical stability
        pivotal_probs[i] = np.exp(logsumexp(log_probs))
    
    # Normalise by the total number of pivotal combinations
    return pivotal_probs / (2**(len(x) - 1))


# Function to solve the system for each Psi
def solve_system(v, phi, Psi_values, Gn, pivotal_prob_func, tolerance=1e-6, max_iterations=1000):
    n = len(v)
    results = {}
    
    # Iterate over each Psi value to find stable strategies
    for Psi in Psi_values:
        x = np.full(n, 0.5)  # Initialise each strategy profile to 0.5 as a starting guess
        # Iteratively update strategies until convergence or maximum iterations are reached
        for iteration in range(max_iterations):
            pivotal_probs = pivotal_prob_func(x)  # Calculate pivotal probabilities for the current profile
            x_new = np.empty(n)
            # Update each strategy based on the influence of neighbours and the system's parameters
            for i in range(n):
                # Calculate the weighted sum of influences for player i
                sum_gx = sum(Gn[i, j] * np.clip(2 * x[j]- 1, 1e-10, 1-1e-10) for j in range(n))
                # Update strategy using the current game parameters
                x_new[i] = np.clip(0.5 + Psi * (v[i] * pivotal_probs[i] + Psi * phi * sum_gx), 1e-10, 1-1e-10)

            # Check for convergence by comparing the new strategy profile with the old
            if np.allclose(x, x_new, atol=tolerance):
                results[Psi] = x_new
                break
            x = x_new
        else:
            # If convergence was not reached, record this Psi as not converging
            print(f"Did not converge for Psi: {Psi}")
            results[Psi] = None

    return results


def compute_jacobian(x, pivotal_prob_func, h=1e-5):
    n = len(x)  # Determine the number of strategies or agents
    jacobian = np.zeros((n, n))  # Initialise the Jacobian matrix with zeros
    
    # Compute the Jacobian by finite difference approximation
    for j in range(n):
        x1 = x.copy()
        x2 = x.copy()
        x1[j] -= h / 2  # Perturb x downwards
        x2[j] += h / 2  # Perturb x upwards
        f1 = pivotal_prob_func(x1)  # Compute function value at x1
        f2 = pivotal_prob_func(x2)  # Compute function value at x2
        # Calculate the derivative (rate of change) for each strategy
        jacobian[:, j] = (f2 - f1) / h

    return jacobian

# Run the model for each Psi value to assess sensitivity and stability
for Psi in Psi_values:
    # Solve the system for current Psi and get strategic profiles
    results = solve_system(v_values, phi, Psi_values, Gn, calculate_pivotal_probabilities)
    if results[Psi] is not None:  # Check if the system converged
        # Compute Jacobian matrix for the converged profiles
        jacobian_matrix = compute_jacobian(results[Psi], calculate_pivotal_probabilities)
        # Transpose the Jacobian matrix to prepare for other computations
        transposed_matrixJ = jacobian_matrix.T  # Transpose for further analysis or processing

        

############################################################################################################################################################################

I = np.eye(n)  # Identity matrix of size n x n
ones_vector = np.ones((n, 1))  # Vector of ones of size n x 1
V_diag = np.diag(V)  # Diagonal elements of the matrix V


# Loop over each Psi value
for Psi in Psi_values:
    # Calculate the value from the phistar function
    phistar_value = phistar(Psi)  # Assuming phistar now only takes Psi, since phi is fixed
    
    # Calculate the matrix inside the brackets more efficiently
    matrix_inside_brackets = I - (phistar_value * G_transpose + (Psi * transposed_matrixJ) @ V_diag)

# Solve the linear system Ax = b for x, where A is matrix_inside_brackets and b is ones_vector.
# This method is preferred over directly computing the inverse of A (A^-1) and then multiplying by b (A^-1 * b)
# because it is more numerically stable and computationally efficient.
# Directly inverting a matrix can lead to large numerical errors especially if the matrix is near singular or poorly conditioned.
# The 'solve' function uses optimized algorithms (like LU decomposition) that are more robust to these issues, providing more accurate results.

    # Solve the linear system instead of inverting the matrix directly for numerical stability
    b_M = solve(matrix_inside_brackets, ones_vector)  # Solve the linear system

    print(f"Psi: {Psi:.2f}, The Katz-Bonacich centrality vector b_M is:")
    print(b_M)
    b_M_array = b_M.flatten()  # Flatten the matrix to a 1D array if needed for further processing
    print("Matrix as array:")
    print(b_M_array)



# There is probably a more efficient way of doing this. Feel free to change it. 

    # Store the entries
    x1.append(b_M_array[15])
    x2.append(b_M_array[143])
    x3.append(b_M_array[221])
    x4.append(b_M_array[145])
    x5.append(b_M_array[126])
    x6.append(b_M_array[23])
    x7.append(b_M_array[333])
    x8.append(b_M_array[3])
    x9.append(b_M_array[410])
    x10.append(b_M_array[299])
    x11.append(b_M_array[1])
    x12.append(b_M_array[147])
    x13.append(b_M_array[291])
    x14.append(b_M_array[101])
    x15.append(b_M_array[121])
    x16.append(b_M_array[29])
    x17.append(b_M_array[397])
    x18.append(b_M_array[9])
    x19.append(b_M_array[421])
    x20.append(b_M_array[234])
    x21.append(b_M_array[55])
    x22.append(b_M_array[66])
    x23.append(b_M_array[77])
    x24.append(b_M_array[99])
    x25.append(b_M_array[88])
    x26.append(b_M_array[148])
    x27.append(b_M_array[111])
    x28.append(b_M_array[222])
    x29.append(b_M_array[332])
    x30.append(b_M_array[402])
  



plt.figure(figsize=(10, 5))
plt.plot(Psi_values, x1)
plt.plot(Psi_values, x2)
plt.plot(Psi_values, x3)
plt.plot(Psi_values, x4)
plt.plot(Psi_values, x5)
plt.plot(Psi_values, x6)
plt.plot(Psi_values, x7)
plt.plot(Psi_values, x8)
plt.plot(Psi_values, x9)
plt.plot(Psi_values, x10)
plt.plot(Psi_values, x11)
plt.plot(Psi_values, x12)
plt.plot(Psi_values, x13)
plt.plot(Psi_values, x14)
plt.plot(Psi_values, x15)
plt.plot(Psi_values, x16)
plt.plot(Psi_values, x17)
plt.plot(Psi_values, x18)
plt.plot(Psi_values, x19)
plt.plot(Psi_values, x20)
plt.plot(Psi_values, x21)
plt.plot(Psi_values, x22)
plt.plot(Psi_values, x23)
plt.plot(Psi_values, x24)
plt.plot(Psi_values, x25)
plt.plot(Psi_values, x26)
plt.plot(Psi_values, x27)
plt.plot(Psi_values, x28)
plt.plot(Psi_values, x29)
plt.plot(Psi_values, x30)
plt.xlabel(r'$\Psi$')                                                   # Here goes the x-axis lable
plt.ylabel('Modified Katz-Bonaich Centrality Measure')                  # Here goes the y-axis lable
plt.title('Modified Katz-Bonacich Centrality Measure vs. Uncertainty')  # Here goes the title 
plt.grid(True)
plt.show()
